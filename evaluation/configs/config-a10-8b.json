{
    "model": "Llama-2-7b",
    "model_path": "/mnt/mcx/models/llama2-7b-chat",
    "num_layers": 32,
    "block_size": 16,
    "max_model_len": 8192,
    "max_num_seqs": 256,
    "max_num_batched_tokens": 8192,
    "tensor_parallel_size": 1,
    "gpu_memory_utilization": 0.8,
    "num_gpu_blocks_override": 800,
    "swap_space": 64,
    "library": "libpacpu-llama2_7b-tp1.so"
}